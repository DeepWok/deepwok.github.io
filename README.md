<center> <h1>Welcome to the DeepWok Lab</h1> </center>

The DeepWok Lab, is an ML research group led by [Dr. Aaron Zhao](https://aaronzhao.me/), where the group members are mainly from Imperial College London and the University of Cambridge.

<center>
<img src="{{site.baseurl}}/deepwok2.png">
</center>



# Members

* [Aaron Zhao](https://aaronzhao.me/)  (Principle Investigator)
* [Cheng Zhang](https://www.linkedin.com/in/cheng-zhang-2aa1061a1/?originalSubdomain=cn) (PhD Student, co-supervised with [Prof. George Constantinides](https://cas.ee.ic.ac.uk/people/gac1/))
* [Victor Zhao](https://victorzxy.github.io/) (PhD Student, co-supervised with [Prof. Pietro Lio](https://www.cl.cam.ac.uk/~pl219/))
* [Zehui Li](https://healthtechweb.com/) (PhD Student, co-supervised with [Prof. Guy-Bart Stan](https://gstan.bg-research.cc.ic.ac.uk/welcome.html#Group_members))

---
# Student Projects

Each year, we run and supervise a number of students for their undergraduate and master projects at Imperial College London and the University of Cambridge. We also run a great number of summer research internships.

## Project Proposals

This is an incomplete list of possible projects for summer/PartII/PartIII/Mphil/MSc/FYP students [last update: 26th Jan].
Some of the links may lead to empty Notion pages; these projects do exist, it's just taking some time to complete the documents.
If you are interested, please email me with your transcript and CV, and the following is the list of our project proposals:

* Note: For Imperial UROP candidates (does not apply to Cambridge UROP since those were naturally co-hosted), I am running out of capacity to accommodate UROP supervision, so now I only offer summer projects that are co-supervised:
	- ~2 positions for **Constrained Hardware Accelerator Design for Machine Learning**


* *Taken* ~~[Efficient Self-supervised Learning](https://pie-ear-389.notion.site/Efficient-Self-supervised-Learning-77ee286a7d264a74972ab31e7ccef116)~~
* *Tentatively Taken* ~~[Sparse training with dynamic lottery ticket](https://pie-ear-389.notion.site/Sparse-training-with-dynamic-lottery-ticket-0a5139b599bc42269e406be205ac76b6)~~
* *Tentatively Taken* ~~[Wide Vision Transformers](https://pie-ear-389.notion.site/Wide-Vision-Transformers-477d606ba68a4097856cdcd59f0e4391)~~
* *Tentatively Taken* ~~[Knowledge Distillation for Vision Transformers](https://pie-ear-389.notion.site/Knowledge-Distillation-for-Vision-Transformers-eb3d0d5a072144ec9538112a25ad31b3)~~
* *Tentatively Taken* ~~[Model backdoors in Multi-Agent RL](https://pie-ear-389.notion.site/Model-backdoors-in-Multi-Agent-RL-20eb3e95b4d2476a83703565689a6d13)~~
* *Taken* ~~[Stable and Diverse DNA sequence design using Diffusion Models](https://www.notion.so/Stable-and-Diverse-DNA-sequence-design-using-Diffusion-Models-49717add86354b238678647da942b6af) (Co-supervised with [Prof. Guy-Bart Stan](https://www.imperial.ac.uk/people/g.stan)'s team as part of the AI4EB program)~~
* [Transformer Quantization Framework](https://www.notion.so/Transformer-Quantization-Framework-cb3530272681413fa403d07064b03c32)
* [Low precision Vision Transformers](https://pie-ear-389.notion.site/Low-precision-Vision-Transformers-f8257f92d3ea4d549e2a5fbdf497f4f4)
* *High priority* [Constrained Hardware Accelerator Design for Machine Learning](https://jianyisphd.notion.site/Constrained-Hardware-Accelerator-Design-for-Machine-Learning-5d9b64ec031d4368bb5fef24abb14630) (Co-supervised with [Jianyi Cheng](https://jianyicheng.github.io/))
* [Boosting Model Compression with Unlabelled Data](https://pie-ear-389.notion.site/Model-Compression-with-unlabelled-data-64a44c8225c34022a36f829e871f96af)
* *High priority* [Neural weather modeling using Graph Representational Learning](https://pie-ear-389.notion.site/Neural-weather-modeling-using-Graph-Representational-Learning-79ff11d0fe664b77818920ecbf967d15)
* [Continuing the Art of Prompting in the Vision Land](https://pie-ear-389.notion.site/Continuing-the-Art-of-Prompting-in-the-Vision-Land-74186f9ece5c48e1939affe78bbf1b14)
* [Feature-level Augmentation](https://www.notion.so/Feature-level-Augmentation-8609e4d1cd7149f496121ab498eb4a86)
* [Dataflow Federated Learning](https://pie-ear-389.notion.site/Dataflow-learning-for-distributed-systems-d5933bf9f27245958de6967d33a410d3)
* [Tricking GPT3 using prompt chaining](https://pie-ear-389.notion.site/Tricking-GPT3-using-chaining-1de895a0d5ce4e07882b9faf0a2e183b)
* [GNN Expressiveness and Graph Generative Models](https://walnut-seeker-d3e.notion.site/GNN-Expressiveness-and-Graph-Generative-Models-e4d0f801c78b4f48a3be9693d01b02cb)


I am also happy to host self-proposed projects if it matches the Lab's research interests. Feel free to contact a.zhao@imperial.ac.uk if you would like to do a project with us!

## Past and Current Students

### Academic Year 2022/2023

* Can Xiao (MSc Project, Imperial College London)
* Sheng Luo (MSc Project, Imperial College London)
* Chuiyu Wang (MSc Project, Imperial College London)
* [Pedro Gimense](https://www.pedrogimenes.co.uk/) (Final Year Project, Imperial College London)
* Nickolaos Ilioudis (Final Year Project, Imperial College London)
* [Issa Bqain](https://issabqain.com/) (Final Year Project, Imperial College London)
* [Tobias Cook](https://www.linkedin.com/in/tobias-cook-542b10250/) (Final Year Project, Imperial College London)
* [Peter Barabas](https://www.linkedin.com/in/peter-barabas-195395230/?trk=people-guest_people_search-card&originalSubdomain=uk) (Final Year Project, Imperial College London)
* [Ritvik Shyam](https://ritvikshyam19.wixsite.com/ritvik-shyam)(Final Year Project, Imperial College London)
* [Harry Knighton](https://www.linkedin.com/in/harry-knighton-971452223/?originalSubdomain=uk) (Part II project, University of Cambridge)
* [Fredrik Ekholm](https://www.linkedin.com/in/fredrik-ekholm-503711146/?originalSubdomain=se) (Part II project, University of Cambridge)
* [Thomas Yuan](https://hk.linkedin.com/in/thomasyuan1) (Part II project, University of Cambridge)
* [Kyra Zhou](https://uk.linkedin.com/in/kyra-zhou) (Part II project, University of Cambridge)

### Academic Year 2021/2022

* [Tim Clifford](https://tim.clifford.lol/) (Summer Research Intern, from University of Cambridge)
* [Joseph Rance](https://www.cst.cam.ac.uk/people/jr879) (Summer Research Intern, from University of Cambridge)
* [Victor Zhao](https://victorzxy.github.io/) (Summer Research Intern, from University of Cambridge)
* [Skye Purchase](https://www.cst.cam.ac.uk/people/atp45) (Summer Research Intern, from University of Cambridge)
* [Cindy Wu](<https://www.linkedin.com/in/cindywux/>) (Summer Research Intern, from University of Cambridge)
* [Guo Yang](https://uk.linkedin.com/in/guo-yang-1b492a21b) (Summer Research Intern, from University of Cambridge)
* [Prisha Satwani](https://uk.linkedin.com/in/prishasatwani) (Summer Research Intern, from LSE)
* [Jason Brown](https://gitlab.com/jrbrown) (Summer Research Intern, from University of Cambridge)

---

# Publication

## Year 2022

Adaptive Channel Sparsity for Federated Learning under System Heterogeneity;
X Gao, D Liao, Y Zhao, C Xu; The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR 2023)

Architectural Backdoors in Neural Networks;
M Bober-Irizar, I Shumailov, Y Zhao, R Mullins, N Papernot; The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR 2023)


## Year 2022

Revisiting Embeddings for Graph Neural Networks; 
S Purchase, Y Zhao, R Mullins; The First Learning on Graphs Conference (LOG 2022)

Wide Attention Is The Way Forward For Transformers;
J R Brown, Y Zhao, I Shumailov, R Mullins;
All Things Attention: Bridging Different Perspectives on Attention, Oral, NeurIPS 2022 Workshop

DARTFormer: Finding The Best Type Of Attention;
J R Brown, Y Zhao, I Shumailov, R Mullins;
ICBINB,  NeurIPS 2022 Workshop

Rapid Model Architecture Adaption for Meta-Learning;
Y Zhao, X Gao, I Shumailov, N Fusi, R Mullins;
Advances in Neural Information Processing Systems 35 (NeurIPS 2022)

DAdaQuant: Doubly-adaptive quantization for communication-efficient Federated Learning;
R HÃ¶nig, Y Zhao, R Mullins;
International Conference on Machine Learning (ICML 2022)
